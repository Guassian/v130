---
title: " Exponential Convergence Rates of Classification Errors on Learning with SGD
  and Random Features "
abstract: " Although kernel methods are widely used in many learning problems, they
  have poor scalability to large datasets. To address this problem, sketching and
  stochastic gradient methods are the most commonly used techniques to derive computationally
  efficient learning algorithms. We consider solving a binary classification problem
  using random features and stochastic gradient descent, both of which are common
  and widely used in practical large-scale problems. Although there are plenty of
  previous works investigating the efficiency of these algorithms in terms of the
  convergence of the objective loss function, these results suggest that the computational
  gain comes at expense of the learning accuracy when dealing with general Lipschitz
  loss functions such as logistic loss. In this study, we analyze the properties of
  these algorithms in terms of the convergence not of the loss function, but the classification
  error under the strong low-noise condition, which reflects a realistic property
  of real-world datasets. We extend previous studies on SGD to a random features setting,
  examining a novel analysis about the error induced by the approximation of random
  features in terms of the distance between the generated hypothesis to show that
  an exponential convergence of the expected classification error is achieved even
  if random features approximation is applied. We demonstrate that the convergence
  rate does not depend on the number of features and there is a significant computational
  benefit in using random features in classification problems under the strong low-noise
  condition. "
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yashima21a
month: 0
tex_title: " Exponential Convergence Rates of Classification Errors on Learning with
  SGD and Random Features "
firstpage: 1954
lastpage: 1962
page: 1954-1962
order: 1954
cycles: false
bibtex_author: Yashima, Shingo and Nitanda, Atsushi and Suzuki, Taiji
author:
- given: Shingo
  family: Yashima
- given: Atsushi
  family: Nitanda
- given: Taiji
  family: Suzuki
date: 2021-03-18
address: 
container-title: Proceedings of The 24th International Conference on Artificial Intelligence
  and Statistics
volume: '130'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 3
  - 18
pdf: http://proceedings.mlr.press/v130/yashima21a/yashima21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v130/yashima21a/yashima21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
