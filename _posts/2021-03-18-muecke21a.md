---
title: " Stochastic Gradient Descent Meets Distribution Regression "
abstract: " Stochastic gradient descent (SGD) provides a simple and efficient way
  to solve a broad range of machine learning problems. Here, we focus on distribution
  regression (DR), involving two stages of sampling: Firstly, we regress from probability
  measures to real-valued responses. Secondly, we sample bags from these distributions
  for utilizing them to solve the overall regression problem. Recently, DR has been
  tackled by applying kernel ridge regression and the learning properties of this
  approach are well understood. However, nothing is known about the learning properties
  of SGD for two stage sampling problems. We fill this gap and provide theoretical
  guarantees for the performance of SGD for DR. Our bounds are optimal in a mini-max
  sense under standard assumptions. "
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: muecke21a
month: 0
tex_title: " Stochastic Gradient Descent Meets Distribution Regression "
firstpage: 2143
lastpage: 2151
page: 2143-2151
order: 2143
cycles: false
bibtex_author: Muecke, Nicole
author:
- given: Nicole
  family: Muecke
date: 2021-03-18
address:
container-title: Proceedings of The 24th International Conference on Artificial Intelligence
  and Statistics
volume: '130'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 3
  - 18
pdf: http://proceedings.mlr.press/v130/muecke21a/muecke21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v130/muecke21a/muecke21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
