---
title: " Problem-Complexity Adaptive Model Selection for Stochastic Linear Bandits "
abstract: " We consider the problem of model selection for two popular stochastic
  linear bandit settings, and propose algorithms that adapts to the unknown problem
  complexity. In the first setting, we consider the $K$ armed mixture bandits, where
  the mean reward of arm $i \\in [K]$ is $\\mu_i+ ⟨\\alpha_{i,t},\\theta^* ⟩$, with
  $\\alpha_{i,t} \\in \\mathbb{R}^d$ being the known context vector and $\\mu_i \\in
  [-1,1]$ and $\\theta^*$ are unknown parameters. We define $\\|\\theta^*\\|$ as the
  problem complexity and consider a sequence of nested hypothesis classes, each positing
  a different upper bound on $\\|\\theta^*\\|$. Exploiting this, we propose Adaptive
  Linear Bandit (ALB), a novel phase based algorithm that adapts to the true problem
  complexity, $\\|\\theta^*\\|$. We show that ALB achieves regret scaling of $\\widetilde{O}(\\|\\theta^*\\|\\sqrt{T})$,
  where $\\|\\theta^*\\|$ is apriori unknown. As a corollary, when $\\theta^*=0$,
  ALB recovers the minimax regret for the simple bandit algorithm without such knowledge
  of $\\theta^*$. ALB is the first algorithm that uses parameter norm as model section
  criteria for linear bandits. Prior state of art algorithms achieve a regret of $\\widetilde{O}(L\\sqrt{T})$,
  where $L$ is the upper bound on $\\|\\theta^*\\|$, fed as an input to the problem.
  In the second setting, we consider the standard linear bandit problem (with possibly
  an infinite number of arms) where the sparsity of $\\theta^*$, denoted by $d^* \\leq
  d$, is unknown to the algorithm. Defining $d^*$ as the problem complexity (similar
  to Foster et. al ’19), we show that ALB achieves $\\widetilde{O}(d^*\\sqrt{T})$
  regret, matching that of an oracle who knew the true sparsity level. This methodology
  is then extended to the case of finitely many arms and similar results are proven.
  We further verify through synthetic and real-data experiments that the performance
  gains are fundamental and not artifacts of mathematical bounds. In particular, we
  show $1.5-3$x drop in cumulative regret over non-adaptive algorithms. "
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ghosh21a
month: 0
tex_title: " Problem-Complexity Adaptive Model Selection for Stochastic Linear Bandits "
firstpage: 1396
lastpage: 1404
page: 1396-1404
order: 1396
cycles: false
bibtex_author: Ghosh, Avishek and Sankararaman, Abishek and Kannan, Ramchandran
author:
- given: Avishek
  family: Ghosh
- given: Abishek
  family: Sankararaman
- given: Ramchandran
  family: Kannan
date: 2021-03-18
address: 
container-title: Proceedings of The 24th International Conference on Artificial Intelligence
  and Statistics
volume: '130'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 3
  - 18
pdf: http://proceedings.mlr.press/v130/ghosh21a/ghosh21a.pdf
extras:
- label: Supplementary ZIP
  link: http://proceedings.mlr.press/v130/ghosh21a/ghosh21a-supp.zip
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
