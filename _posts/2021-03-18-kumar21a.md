---
title: " The Teaching Dimension of Kernel Perceptron "
abstract: " Algorithmic machine teaching has been studied under the linear setting
  where exact teaching is possible. However, little is known for teaching nonlinear
  learners. Here, we establish the sample complexity of teaching, aka teaching dimension,
  for kernelized perceptrons for different families of feature maps. As a warm-up,
  we show that the teaching complexity is $\\Theta(d)$ for the exact teaching of linear
  perceptrons in $\\mathbb{R}^d$, and $\\Theta(d^k)$ for kernel perceptron with a
  polynomial kernel of order $k$. Furthermore, under certain smooth assumptions on
  the data distribution, we establish a rigorous bound on the complexity for approximately
  teaching a Gaussian kernel perceptron. We provide numerical examples of the optimal
  (approximate) teaching set under several canonical settings for linear, polynomial
  and Gaussian kernel perceptions. "
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kumar21a
month: 0
tex_title: " The Teaching Dimension of Kernel Perceptron "
firstpage: 2071
lastpage: 2079
page: 2071-2079
order: 2071
cycles: false
bibtex_author: Kumar, Akash and Zhang, Hanqi and Singla, Adish and Chen, Yuxin
author:
- given: Akash
  family: Kumar
- given: Hanqi
  family: Zhang
- given: Adish
  family: Singla
- given: Yuxin
  family: Chen
date: 2021-03-18
address:
container-title: Proceedings of The 24th International Conference on Artificial Intelligence
  and Statistics
volume: '130'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 3
  - 18
pdf: http://proceedings.mlr.press/v130/kumar21a/kumar21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v130/kumar21a/kumar21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
