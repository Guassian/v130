---
title: " Kernel regression in high dimensions: Refined analysis beyond double descent "
abstract: " In this paper, we provide a precise characterization of generalization
  properties of high dimensional kernel ridge regression across the under- and over-parameterized
  regimes, depending on whether the number of training data n exceeds the feature
  dimension d. By establishing a bias-variance decomposition of the expected excess
  risk, we show that, while the bias is (almost) independent of d and monotonically
  decreases with n, the variance depends on n,d and can be unimodal or monotonically
  decreasing under different regularization schemes. Our refined analysis goes beyond
  the double descent theory by showing that, depending on the data eigen-profile and
  the level of regularization, the kernel regression risk curve can be a double-descent-like,
  bell-shaped, or monotonic function of n. Experiments on synthetic and real data
  are conducted to support our theoretical findings. "
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: liu21b
month: 0
tex_title: " Kernel regression in high dimensions: Refined analysis beyond double
  descent "
firstpage: 649
lastpage: 657
page: 649-657
order: 649
cycles: false
bibtex_author: Liu, Fanghui and Liao, Zhenyu and Suykens, Johan
author:
- given: Fanghui
  family: Liu
- given: Zhenyu
  family: Liao
- given: Johan
  family: Suykens
date: 2021-03-18
address:
container-title: Proceedings of The 24th International Conference on Artificial Intelligence
  and Statistics
volume: '130'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 3
  - 18
pdf: http://proceedings.mlr.press/v130/liu21b/liu21b.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v130/liu21b/liu21b-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
