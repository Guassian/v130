---
title: " Local SGD: Unified Theory and New Efficient Methods "
abstract: " We present a unified framework for analyzing local SGD methods in the
  convex and strongly convex regimes for distributed/federated training of supervised
  machine learning models. We recover several known methods as a special case of our
  general framework, including Local SGD/FedAvg, SCAFFOLD, and several variants of
  SGD not originally designed for federated learning. Our framework covers both the
  identical and heterogeneous data settings, supports both random and deterministic
  number of local steps, and can work with a wide array of local stochastic gradient
  estimators, including shifted estimators which are able to adjust the fixed points
  of local iterations for faster convergence. As an application of our framework,
  we develop multiple novel FL optimizers which are superior to existing methods.
  In particular, we develop the first linearly converging local SGD method which does
  not require any data homogeneity or other strong assumptions. "
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: gorbunov21a
month: 0
tex_title: " Local SGD: Unified Theory and New Efficient Methods "
firstpage: 3556
lastpage: 3564
page: 3556-3564
order: 3556
cycles: false
bibtex_author: Gorbunov, Eduard and Hanzely, Filip and Richtarik, Peter
author:
- given: Eduard
  family: Gorbunov
- given: Filip
  family: Hanzely
- given: Peter
  family: Richtarik
date: 2021-03-18
address:
container-title: Proceedings of The 24th International Conference on Artificial Intelligence
  and Statistics
volume: '130'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 3
  - 18
pdf: http://proceedings.mlr.press/v130/gorbunov21a/gorbunov21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v130/gorbunov21a/gorbunov21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
