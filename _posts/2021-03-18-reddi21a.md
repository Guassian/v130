---
title: " RankDistil: Knowledge Distillation for Ranking "
abstract: " Knowledge distillation is an approach to improve the performance of a
  student model by using the knowledge of a complex teacher. Despite its success in
  several deep learning applications, the study of distillation is mostly confined
  to classification settings. In particular, the use of distillation in top-k ranking
  settings, where the goal is to rank k most relevant items correctly, remains largely
  unexplored. In this paper, we study such ranking problems through the lens of distillation.
  We present a distillation framework for top-k ranking and draw connections with
  the existing ranking methods. The core idea of this framework is to preserve the
  ranking at the top by matching the order of items of student and teacher, while
  penalizing large scores for items ranked low by the teacher. Building on this, we
  develop a novel distillation approach, RankDistil, specifically catered towards
  ranking problems with a large number of items to rank, and establish statistical
  basis for the method. Finally, we conduct experiments which demonstrate that RankDistil
  yields benefits over commonly used baselines for ranking problems. "
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: reddi21a
month: 0
tex_title: " RankDistil: Knowledge Distillation for Ranking "
firstpage: 2368
lastpage: 2376
page: 2368-2376
order: 2368
cycles: false
bibtex_author: Reddi, Sashank and Kumar Pasumarthi, Rama and Menon, Aditya and Singh
  Rawat, Ankit and Yu, Felix and Kim, Seungyeon and Veit, Andreas and Kumar, Sanjiv
author:
- given: Sashank
  family: Reddi
- given: Rama
  family: Kumar Pasumarthi
- given: Aditya
  family: Menon
- given: Ankit
  family: Singh Rawat
- given: Felix
  family: Yu
- given: Seungyeon
  family: Kim
- given: Andreas
  family: Veit
- given: Sanjiv
  family: Kumar
date: 2021-03-18
address: 
container-title: Proceedings of The 24th International Conference on Artificial Intelligence
  and Statistics
volume: '130'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 3
  - 18
pdf: http://proceedings.mlr.press/v130/reddi21a/reddi21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v130/reddi21a/reddi21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
