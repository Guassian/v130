---
title: " The Sample Complexity of Meta Sparse Regression "
abstract: " This paper addresses the meta-learning problem in sparse linear regression
  with infinite tasks. We assume that the learner can access several similar tasks.
  The goal of the learner is to transfer knowledge from the prior tasks to a similar
  but novel task. For $p$ parameters, size of the support set $k$, and $l$ samples
  per task, we show that $T \\in O((k \\log (p-k)) / l)$ tasks are sufficient in order
  to recover the common support of all tasks. With the recovered support, we can greatly
  reduce the sample complexity for estimating the parameter of the novel task, i.e.,
  $l \\in O(1)$ with respect to $T$ and $p$. We also prove that our rates are minimax
  optimal. A key difference between meta-learning and the classical multi-task learning,
  is that meta-learning focuses only on the recovery of the parameters of the novel
  task, while multi-task learning estimates the parameter of all tasks, which requires
  $l$ to grow with $T$. Instead, our efficient meta-learning estimator allows for
  $l$ to be constant with respect to $T$ (i.e., few-shot learning). "
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wang21f
month: 0
tex_title: " The Sample Complexity of Meta Sparse Regression "
firstpage: 2323
lastpage: 2331
page: 2323-2331
order: 2323
cycles: false
bibtex_author: Wang, Zhanyu and Honorio, Jean
author:
- given: Zhanyu
  family: Wang
- given: Jean
  family: Honorio
date: 2021-03-18
address: 
container-title: Proceedings of The 24th International Conference on Artificial Intelligence
  and Statistics
volume: '130'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 3
  - 18
pdf: http://proceedings.mlr.press/v130/wang21f/wang21f.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v130/wang21f/wang21f-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
