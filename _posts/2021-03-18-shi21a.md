---
title: " Active Learning with Maximum Margin Sparse Gaussian Processes "
abstract: ' We present a maximum-margin sparse Gaussian Process (MM-SGP) for active
  learning (AL) of classification models for multi-class problems. The proposed model
  makes novel extensions to a GP by integrating maximum-margin constraints into its
  learning process, aiming to further improve its predictive power while keeping its
  inherent capability for uncertainty quantification. The MM constraints ensure small
  "effective size" of the model, which allows MM-SGP to provide good predictive performance
  by using limited "active" data samples, a critical property for AL. Furthermore,
  as a Gaussian process model, MM-SGP will output both the predicted class distribution
  and the predictive variance, both of which are essential for defining a sampling
  function effective to improve the decision boundaries of a large number of classes
  simultaneously. Finally, the sparse nature of MM-SGP ensures that it can be efficiently
  trained by solving a low-rank convex dual problem. Experiment results on both synthetic
  and real-world datasets show the effectiveness and efficiency of the proposed AL
  model. '
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: shi21a
month: 0
tex_title: " Active Learning with Maximum Margin Sparse Gaussian Processes "
firstpage: 406
lastpage: 414
page: 406-414
order: 406
cycles: false
bibtex_author: Shi, Weishi and Yu, Qi
author:
- given: Weishi
  family: Shi
- given: Qi
  family: Yu
date: 2021-03-18
address:
container-title: Proceedings of The 24th International Conference on Artificial Intelligence
  and Statistics
volume: '130'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 3
  - 18
pdf: http://proceedings.mlr.press/v130/shi21a/shi21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v130/shi21a/shi21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
