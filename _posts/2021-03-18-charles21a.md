---
title: " Convergence and Accuracy Trade-Offs in Federated Learning and Meta-Learning "
abstract: " We study a family of algorithms, which we refer to as local update methods,
  generalizing many federated and meta-learning algorithms. We prove that for quadratic
  models, local update methods are equivalent to first-order optimization on a surrogate
  loss we exactly characterize. Moreover, fundamental algorithmic choices (such as
  learning rates) explicitly govern a trade-off between the condition number of the
  surrogate loss and its alignment with the true loss. We derive novel convergence
  rates showcasing these trade-offs and highlight their importance in communication-limited
  settings. Using these insights, we are able to compare local update methods based
  on their convergence/accuracy trade-off, not just their convergence to critical
  points of the empirical loss. Our results shed new light on a broad range of phenomena,
  including the efficacy of server momentum in federated learning and the impact of
  proximal client updates. "
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: charles21a
month: 0
tex_title: " Convergence and Accuracy Trade-Offs in Federated Learning and Meta-Learning "
firstpage: 2575
lastpage: 2583
page: 2575-2583
order: 2575
cycles: false
bibtex_author: Charles, Zachary and Kone\v{c}n\'y, Jakub
author:
- given: Zachary
  family: Charles
- given: Jakub
  family: Konečný
date: 2021-03-18
address:
container-title: Proceedings of The 24th International Conference on Artificial Intelligence
  and Statistics
volume: '130'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 3
  - 18
pdf: http://proceedings.mlr.press/v130/charles21a/charles21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v130/charles21a/charles21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
