---
title: " Online Sparse Reinforcement Learning "
abstract: " We investigate the hardness of online reinforcement learning in sparse
  linear Markov decision process (MDP), with a special focus on the high-dimensional
  regime where the ambient dimension is larger than the number of episodes. Our contribution
  is two-fold. First, we provide a lower bound showing that linear regret is generally
  unavoidable, even if there exists a policy that collects well-conditioned data.
  Second, we show that if the learner has oracle access to a policy that collects
  well-conditioned data, then a variant of Lasso fitted Q-iteration enjoys a regret
  of $O(N^{2/3})$ where $N$ is the number of episodes. "
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: hao21a
month: 0
tex_title: " Online Sparse Reinforcement Learning "
firstpage: 316
lastpage: 324
page: 316-324
order: 316
cycles: false
bibtex_author: Hao, Botao and Lattimore, Tor and Szepesvari, Csaba and Wang, Mengdi
author:
- given: Botao
  family: Hao
- given: Tor
  family: Lattimore
- given: Csaba
  family: Szepesvari
- given: Mengdi
  family: Wang
date: 2021-03-18
address: 
container-title: Proceedings of The 24th International Conference on Artificial Intelligence
  and Statistics
volume: '130'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 3
  - 18
pdf: http://proceedings.mlr.press/v130/hao21a/hao21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v130/hao21a/hao21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
