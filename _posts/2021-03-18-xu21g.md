---
title: " Meta Learning in the Continuous Time Limit "
abstract: " In this paper, we establish the ordinary differential equation (ODE) that
  underlies the training dynamics of Model-Agnostic Meta-Learning (MAML). Our continuous-time
  limit view of the process eliminates the influence of the manually chosen step size
  of gradient descent and includes the existing gradient descent training algorithm
  as a special case that results from a specific discretization. We show that the
  MAML ODE enjoys a linear convergence rate to an approximate stationary point of
  the MAML loss function for strongly convex task losses, even when the corresponding
  MAML loss is non-convex. Moreover, through the analysis of the MAML ODE, we propose
  a new BI-MAML training algorithm that reduces the computational burden associated
  with existing MAML training methods, and empirical experiments are performed to
  showcase the superiority of our proposed methods in the rate of convergence with
  respect to the vanilla MAML algorithm. "
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: xu21g
month: 0
tex_title: " Meta Learning in the Continuous Time Limit "
firstpage: 3052
lastpage: 3060
page: 3052-3060
order: 3052
cycles: false
bibtex_author: Xu, Ruitu and Chen, Lin and Karbasi, Amin
author:
- given: Ruitu
  family: Xu
- given: Lin
  family: Chen
- given: Amin
  family: Karbasi
date: 2021-03-18
address:
container-title: Proceedings of The 24th International Conference on Artificial Intelligence
  and Statistics
volume: '130'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 3
  - 18
pdf: http://proceedings.mlr.press/v130/xu21g/xu21g.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v130/xu21g/xu21g-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
