---
title: " Minimax Model Learning "
abstract: " We present a novel off-policy loss function for learning a transition
  model in model-based reinforcement learning. Notably, our loss is derived from the
  off-policy policy evaluation objective with an emphasis on correcting distribution
  shift. Compared to previous model-based techniques, our approach allows for greater
  robustness under model misspecification or distribution shift induced by learning/evaluating
  policies that are distinct from the data-generating policy. We provide a theoretical
  analysis and show empirical improvements over existing model-based off-policy evaluation
  methods. We provide further analysis showing our loss can be used for off-policy
  optimization (OPO) and demonstrate its integration with more recent improvements
  in OPO. "
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: voloshin21a
month: 0
tex_title: " Minimax Model Learning "
firstpage: 1612
lastpage: 1620
page: 1612-1620
order: 1612
cycles: false
bibtex_author: Voloshin, Cameron and Jiang, Nan and Yue, Yisong
author:
- given: Cameron
  family: Voloshin
- given: Nan
  family: Jiang
- given: Yisong
  family: Yue
date: 2021-03-18
address:
container-title: Proceedings of The 24th International Conference on Artificial Intelligence
  and Statistics
volume: '130'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 3
  - 18
pdf: http://proceedings.mlr.press/v130/voloshin21a/voloshin21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v130/voloshin21a/voloshin21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
