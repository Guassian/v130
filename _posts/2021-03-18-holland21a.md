---
title: " Robustness and scalability under heavy tails, without strong convexity "
abstract: " Real-world data is laden with outlying values. The challenge for machine
  learning is that the learner typically has no prior knowledge of whether the feedback
  it receives (losses, gradients, etc.) will be heavy-tailed or not. In this work,
  we study a simple, cost-efficient algorithmic strategy that can be leveraged when
  both losses and gradients can be heavy-tailed. The core technique introduces a simple
  robust validation sub-routine, which is used to boost the confidence of inexpensive
  gradient-based sub-processes. Compared with recent robust gradient descent methods
  from the literature, dimension dependence (both risk bounds and cost) is substantially
  improved, without relying upon strong convexity or expensive per-step robustification.
  We also empirically show that the proposed procedure cannot simply be replaced with
  naive cross-validation. "
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: holland21a
month: 0
tex_title: " Robustness and scalability under heavy tails, without strong convexity "
firstpage: 865
lastpage: 873
page: 865-873
order: 865
cycles: false
bibtex_author: Holland, Matthew
author:
- given: Matthew
  family: Holland
date: 2021-03-18
address:
container-title: Proceedings of The 24th International Conference on Artificial Intelligence
  and Statistics
volume: '130'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 3
  - 18
pdf: http://proceedings.mlr.press/v130/holland21a/holland21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v130/holland21a/holland21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
