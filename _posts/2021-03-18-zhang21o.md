---
title: " Meta-Learning Divergences for Variational Inference "
abstract: " Variational inference (VI) plays an essential role in approximate Bayesian
  inference due to its computational efficiency and broad applicability. Crucial to
  the performance of VI is the selection of the associated divergence measure, as
  VI approximates the intractable distribution by minimizing this divergence. In this
  paper we propose a meta-learning algorithm to learn the divergence metric suited
  for the task of interest, automating the design of VI methods. In addition, we learn
  the initialization of the variational parameters without additional cost when our
  method is deployed in the few-shot learning scenarios. We demonstrate our approach
  outperforms standard VI on Gaussian mixture distribution approximation, Bayesian
  neural network regression, image generation with variational autoencoders and recommender
  systems with a partial variational autoencoder. "
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhang21o
month: 0
tex_title: " Meta-Learning Divergences for Variational Inference "
firstpage: 4024
lastpage: 4032
page: 4024-4032
order: 4024
cycles: false
bibtex_author: Zhang, Ruqi and Li, Yingzhen and De Sa, Christopher and Devlin, Sam
  and Zhang, Cheng
author:
- given: Ruqi
  family: Zhang
- given: Yingzhen
  family: Li
- given: Christopher
  family: De Sa
- given: Sam
  family: Devlin
- given: Cheng
  family: Zhang
date: 2021-03-18
address:
container-title: Proceedings of The 24th International Conference on Artificial Intelligence
  and Statistics
volume: '130'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 3
  - 18
pdf: http://proceedings.mlr.press/v130/zhang21o/zhang21o.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v130/zhang21o/zhang21o-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
