---
title: " Fisher Auto-Encoders "
abstract: " It has been conjectured that the Fisher divergence is more robust to model
  uncertainty than the conventional Kullback-Leibler (KL) divergence. This motivates
  the design of a new class of robust generative auto-encoders (AE) referred to as
  Fisher auto-encoders. Our approach is to design Fisher AEs by minimizing the Fisher
  divergence between the intractable joint distribution of observed data and latent
  variables, with that of the postulated/modeled joint distribution. In contrast to
  KL-based variational AEs (VAEs), the Fisher AE can exactly quantify the distance
  between the true and the model-based posterior distributions. Qualitative and quantitative
  results are provided on both MNIST and celebA datasets demonstrating the competitive
  performance of Fisher AEs in terms of robustness compared to other AEs such as VAEs
  and Wasserstein AEs. "
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: elkhalil21a
month: 0
tex_title: " Fisher Auto-Encoders "
firstpage: 352
lastpage: 360
page: 352-360
order: 352
cycles: false
bibtex_author: Elkhalil, Khalil and Hasan, Ali and Ding, Jie and Farsiu, Sina and
  Tarokh, Vahid
author:
- given: Khalil
  family: Elkhalil
- given: Ali
  family: Hasan
- given: Jie
  family: Ding
- given: Sina
  family: Farsiu
- given: Vahid
  family: Tarokh
date: 2021-03-18
address:
container-title: Proceedings of The 24th International Conference on Artificial Intelligence
  and Statistics
volume: '130'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 3
  - 18
pdf: http://proceedings.mlr.press/v130/elkhalil21a/elkhalil21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v130/elkhalil21a/elkhalil21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
