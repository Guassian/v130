---
title: " A Contraction Approach to Model-based Reinforcement Learning "
abstract: " Despite its experimental success, Model-based Reinforcement Learning still
  lacks a complete theoretical understanding. To this end, we analyze the error in
  the cumulative reward using a contraction approach. We consider both stochastic
  and deterministic state transitions for continuous (non-discrete) state and action
  spaces. This approach doesnâ€™t require strong assumptions and can recover the typical
  quadratic error to the horizon. We prove that branched rollouts can reduce this
  error and are essential for deterministic transitions to have a Bellman contraction.
  Our analysis of policy mismatch error also applies to Imitation Learning. In this
  case, we show that GAN-type learning has an advantage over Behavioral Cloning when
  its discriminator is well-trained. "
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: fan21a
month: 0
tex_title: " A Contraction Approach to Model-based Reinforcement Learning "
firstpage: 325
lastpage: 333
page: 325-333
order: 325
cycles: false
bibtex_author: Fan, Ting-Han and Ramadge, Peter
author:
- given: Ting-Han
  family: Fan
- given: Peter
  family: Ramadge
date: 2021-03-18
address:
container-title: Proceedings of The 24th International Conference on Artificial Intelligence
  and Statistics
volume: '130'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 3
  - 18
pdf: http://proceedings.mlr.press/v130/fan21a/fan21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v130/fan21a/fan21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
