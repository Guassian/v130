---
title: " One-pass Stochastic Gradient Descent in overparametrized two-layer neural
  networks "
abstract: " There has been a recent surge of interest in understanding the convergence
  of gradient descent (GD) and stochastic gradient descent (SGD) in overparameterized
  neural networks. Most previous work assumes that the training data is provided a
  priori in a batch, while less attention has been paid to the important setting where
  the training data arrives in a stream. In this paper, we study the streaming data
  setup and show that with overparamterization and random initialization, the prediction
  error of two-layer neural networks under one-pass SGD converges in expectation.
  The convergence rate depends on the eigen-decomposition of the integral operator
  associated with the so-called neural tangent kernel (NTK). A key step of our analysis
  is to show a random kernel function converges to the NTK with high probability using
  the VC dimension and McDiarmidâ€™s inequality. "
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhu21d
month: 0
tex_title: " One-pass Stochastic Gradient Descent in overparametrized two-layer neural
  networks "
firstpage: 3673
lastpage: 3681
page: 3673-3681
order: 3673
cycles: false
bibtex_author: Zhu, Hanjing and Xu, Jiaming
author:
- given: Hanjing
  family: Zhu
- given: Jiaming
  family: Xu
date: 2021-03-18
address:
container-title: Proceedings of The 24th International Conference on Artificial Intelligence
  and Statistics
volume: '130'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 3
  - 18
pdf: http://proceedings.mlr.press/v130/zhu21d/zhu21d.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v130/zhu21d/zhu21d-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
