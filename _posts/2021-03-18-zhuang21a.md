---
title: " No-Regret Reinforcement Learning with Heavy-Tailed Rewards "
abstract: " Reinforcement learning algorithms typically assume rewards to be sampled
  from light-tailed distributions, such as Gaussian or bounded. However, a wide variety
  of real-world systems generate rewards that follow heavy-tailed distributions. We
  consider such scenarios in the setting of undiscounted reinforcement learning. By
  constructing a lower bound, we show that the difficulty of learning heavy-tailed
  rewards asymptotically dominates the difficulty of learning transition probabilities.
  Leveraging techniques from robust mean estimation, we propose Heavy-UCRL2 and Heavy-Q-Learning,
  and show that they achieve near-optimal regret bounds in this setting. Our algorithms
  also naturally generalize to deep reinforcement learning applications; we instantiate
  Heavy-DQN as an example of this. We demonstrate that all of our algorithms outperform
  baselines on both synthetic MDPs and standard RL benchmarks. "
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhuang21a
month: 0
tex_title: " No-Regret Reinforcement Learning with Heavy-Tailed Rewards "
firstpage: 3385
lastpage: 3393
page: 3385-3393
order: 3385
cycles: false
bibtex_author: Zhuang, Vincent and Sui, Yanan
author:
- given: Vincent
  family: Zhuang
- given: Yanan
  family: Sui
date: 2021-03-18
address: 
container-title: Proceedings of The 24th International Conference on Artificial Intelligence
  and Statistics
volume: '130'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 3
  - 18
pdf: http://proceedings.mlr.press/v130/zhuang21a/zhuang21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
