---
title: " Noisy Gradient Descent Converges to Flat Minima for Nonconvex Matrix Factorization "
abstract: " Numerous empirical evidences have corroborated the importance of noise
  in nonconvex optimization problems. The theory behind such empirical observations,
  however, is still largely unknown. This paper studies this fundamental problem through
  investigating the nonconvex rectangular matrix factorization problem, which has
  infinitely many global minima due to rotation and scaling invariance. Hence, gradient
  descent (GD) can converge to any optimum, depending on the initialization. In contrast,
  we show that a perturbed form of GD with an arbitrary initialization converges to
  a global optimum that is uniquely determined by the injected noise. Our result implies
  that the noise imposes implicit bias towards certain optima. Numerical experiments
  are provided to support our theory. "
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: liu21e
month: 0
tex_title: " Noisy Gradient Descent Converges to Flat Minima for Nonconvex Matrix
  Factorization "
firstpage: 1891
lastpage: 1899
page: 1891-1899
order: 1891
cycles: false
bibtex_author: Liu, Tianyi and Li, Yan and Wei, Song and Zhou, Enlu and Zhao, Tuo
author:
- given: Tianyi
  family: Liu
- given: Yan
  family: Li
- given: Song
  family: Wei
- given: Enlu
  family: Zhou
- given: Tuo
  family: Zhao
date: 2021-03-18
address: 
container-title: Proceedings of The 24th International Conference on Artificial Intelligence
  and Statistics
volume: '130'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 3
  - 18
pdf: http://proceedings.mlr.press/v130/liu21e/liu21e.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v130/liu21e/liu21e-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
