---
title: " CADA: Communication-Adaptive Distributed Adam "
abstract: " Stochastic gradient descent (SGD) has taken the stage as the primary workhorse
  for largescale machine learning. It is often used with its adaptive variants such
  as AdaGrad, Adam, and AMSGrad. This paper proposes an adaptive stochastic gradient
  descent method for distributed machine learning, which can be viewed as the communicationadaptive
  counterpart of the celebrated Adam method â€” justifying its name CADA. The key components
  of CADA are a set of new rules tailored for adaptive stochastic gradients that can
  be implemented to save communication upload. The new algorithms adaptively reuse
  the stale Adam gradients, thus saving communication, and still have convergence
  rates comparable to original Adam. In numerical experiments, CADA achieves impressive
  empirical performance in terms of total communication round reduction. "
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chen21a
month: 0
tex_title: " CADA: Communication-Adaptive Distributed Adam "
firstpage: 613
lastpage: 621
page: 613-621
order: 613
cycles: false
bibtex_author: Chen, Tianyi and Guo, Ziye and Sun, Yuejiao and Yin, Wotao
author:
- given: Tianyi
  family: Chen
- given: Ziye
  family: Guo
- given: Yuejiao
  family: Sun
- given: Wotao
  family: Yin
date: 2021-03-18
address: 
container-title: Proceedings of The 24th International Conference on Artificial Intelligence
  and Statistics
volume: '130'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 3
  - 18
pdf: http://proceedings.mlr.press/v130/chen21a/chen21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v130/chen21a/chen21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
