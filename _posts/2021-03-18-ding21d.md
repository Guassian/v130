---
title: " Provably Efficient Safe Exploration via Primal-Dual Policy Optimization "
abstract: " We study the safe reinforcement learning problem using the constrained
  Markov decision processes in which an agent aims to maximize the expected total
  reward subject to a safety constraint on the expected total value of a utility function.
  We focus on an episodic setting with the function approximation where the Markov
  transition kernels have a linear structure but do not impose any additional assumptions
  on the sampling model. Designing safe reinforcement learning algorithms with provable
  computational and statistical efficiency is particularly challenging under this
  setting because of the need to incorporate both the safety constraint and the function
  approximation into the fundamental exploitation/exploration tradeoff. To this end,
  we present an \\underline{O}ptimistic \\underline{P}rimal-\\underline{D}ual Proximal
  Policy \\underline{OP}timization \\mbox{(OPDOP)} algorithm where the value function
  is estimated by combining the least-squares policy evaluation and an additional
  bonus term for safe exploration. We prove that the proposed algorithm achieves an
  $\\tilde{O}(d H^{2.5}\\sqrt{T})$ regret and an $\\tilde{O}(d H^{2.5}\\sqrt{T})$
  constraint violation, where $d$ is the dimension of the feature mapping, $H$ is
  the horizon of each episode, and $T$ is the total number of steps. These bounds
  hold when the reward/utility functions are fixed but the feedback after each episode
  is bandit. Our bounds depend on the capacity of the state-action space only through
  the dimension of the feature mapping and thus our results hold even when the number
  of states goes to infinity. To the best of our knowledge, we provide the first provably
  efficient online policy optimization algorithm for constrained Markov decision processes
  in the function approximation setting, with safe exploration. "
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ding21d
month: 0
tex_title: " Provably Efficient Safe Exploration via Primal-Dual Policy Optimization "
firstpage: 3304
lastpage: 3312
page: 3304-3312
order: 3304
cycles: false
bibtex_author: Ding, Dongsheng and Wei, Xiaohan and Yang, Zhuoran and Wang, Zhaoran
  and Jovanovic, Mihailo
author:
- given: Dongsheng
  family: Ding
- given: Xiaohan
  family: Wei
- given: Zhuoran
  family: Yang
- given: Zhaoran
  family: Wang
- given: Mihailo
  family: Jovanovic
date: 2021-03-18
address: 
container-title: Proceedings of The 24th International Conference on Artificial Intelligence
  and Statistics
volume: '130'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 3
  - 18
pdf: http://proceedings.mlr.press/v130/ding21d/ding21d.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v130/ding21d/ding21d-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
